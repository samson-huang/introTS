mean.dist<-airline.mean%>%
dist()
mean.mds<-cmdscale(mean.dist)%>%
data.frame()
names(mean.mds)<-c("Score1","Score2")
plot(mean.mds,type="n",xlim=c(-8,6),ylim=c(-5,6))
text(mean.mds,row.names(mean.mds),cex=1)
mean.rank<-lapply(airline.mean,function(x) factor(rank(x),ordered=T))%>%
data.frame()
glimpse(mean.rank)
library(cluster)
mean.rank.gower<-daisy(mean.rank,metric="gower")
library(MASS)
mean.mds.gower<-isoMDS(mean.rank.gower)
plot(mean.mds.gower$points,type="n",xlim=c(-0.5,0.5),ylim=c(-0.2,0.4))
text(mean.mds.gower$points,row.names(mean.mds),cex=1)
dat<-read.table('SegData.csv',,sep=',',header=TRUE)
dat<-subset(dat,store_exp>0 & online_exp>0 & age<100)
cor(dat$age,dat$store_exp)
cor(dat$age,dat$online_exp,method="spearman")
cor(dat$age,dat$online_exp,method="kendall")
smoother <- loess(dat$online_exp ~ dat$age)
pred.smoother<-predict(smoother, dat$age)
rss<-sum((dat$online_exp-pred.smoother)^2)
tss<-sum((dat$online_exp-mean(dat$online_exp))^2)
(r2<-1-rss/tss)
library(lattice)
xyplot(dat$online_exp ~ dat$age,
type = c("p", "smooth"),
xlab = "age",
ylab = "online_exp")
library(ggplot2)
ggplot(dat,aes(age,online_exp))+geom_smooth()
TrainXtrans<-dat[,grep("Q",names(dat))]
TrainY<-dat$store_exp+dat$online_exp
loessResults <- filterVarImp(x = TrainXtrans, y = TrainY, nonpara = TRUE)
head(loessResults)
t.test(online_exp~gender,data=dat)
anova(lm(online_exp~segment,data=dat))
vars<-c("age","income","store_exp","online_exp")
x<-subset(dat,select=vars)
y<-dat$house
rocValues <- filterVarImp(x = x, y = y)
rocValues
y<-dat$segment
rocValues <- filterVarImp(x = x, y = y)
rocValues
library(CORElearn)
infoCore(what="attrEvalReg")
infoCore(what="attrEval")
dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
dat=filter(dat,!is.na(income))
xtrain=dat%>%
dplyr::select(num_range("Q", 1:10))
ytrain=dat$income
reliefValues = attrEval(ytrain ~ ., data = xtrain,
## 有许多计算Relief的方法，这里将k个邻近样本设置为等权重
## 更多信息键入?attrEval
estimator = "RReliefFequalK",
## 测试的数目：
ReliefIterations = 50)
head(reliefValues)
library(AppliedPredictiveModeling)
perm = permuteRelief(x = xtrain, y = ytrain,
nperm = 500,
estimator = "RReliefFequalK",
ReliefIterations = 50)
head(perm$permutations)
lattice::histogram(~ value|Predictor,data = perm$permutations)
head(perm$standardized)
library(leaps)
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
ytrain<-sim.dat$income
xtrain<-dplyr::select(sim.dat,store_exp,online_exp,store_trans,online_trans,
Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10)
regfit.full=regsubsets(ytrain~.,xtrain)
summary(regfit.full)
regfit.full=regsubsets(ytrain~.,xtrain,nvmax=10)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$adjr2
par(mfrow=c(2,2))
plot(reg.summary$adjr2,xlab="变量个数",family ="Songti SC",ylab="调整R2",type="l")
idx<-which.max(reg.summary$adjr2)
points(idx,reg.summary$adjr2[idx],col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="变量个数",family ="Songti SC",ylab="Cp",type="l")
idx<-which.min(reg.summary$cp)
points(idx,reg.summary$cp[idx],col="red",cex=2,pch=20)
plot(reg.summary$bic,xlab="变量个数",family ="Songti SC",ylab="BIC",type="l")
idx<-which.min(reg.summary$bic)
points(idx,reg.summary$bic[idx],col="red",cex=2,pch=20)
plot(reg.summary$rss,xlab="变量个数",family ="Songti SC",ylab="RSS",type="l")
idx<-which.min(reg.summary$rss)
points(idx,reg.summary$rss[idx],col="red",cex=2,pch=20)
library(leaps)
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
ytrain<-sim.dat$income
xtrain<-dplyr::select(sim.dat,store_exp,online_exp,store_trans,online_trans,
Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10)
regfit.fwd=regsubsets(ytrain~.,xtrain,nvmax=10,method="forward")
reg.summary=summary(regfit.fwd)
par(mfrow=c(2,2))
plot(reg.summary$adjr2,xlab="变量个数",family ="Songti SC",ylab="调整R2",type="l")
idx<-which.max(reg.summary$adjr2)
points(idx,reg.summary$adjr2[idx],col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="变量个数",family ="Songti SC",ylab="Cp",type="l")
idx<-which.min(reg.summary$cp)
points(idx,reg.summary$cp[idx],col="red",cex=2,pch=20)
plot(reg.summary$bic,xlab="变量个数",family ="Songti SC",ylab="BIC",type="l")
idx<-which.min(reg.summary$bic)
points(idx,reg.summary$bic[idx],col="red",cex=2,pch=20)
plot(reg.summary$rss,xlab="变量个数",family ="Songti SC",ylab="RSS",type="l")
idx<-which.min(reg.summary$rss)
points(idx,reg.summary$rss[idx],col="red",cex=2,pch=20)
regfit.bwd=regsubsets(ytrain~.,xtrain,nvmax=10,method="backward")
reg.summary=summary(regfit.bwd)
library(MASS)
fit=lm(ytrain~.,data=xtrain)
stepAIC(fit)
load("D:/quant/20180309.RData")
head(test)
library(sqldf)
newdf<-sqldf("select pct_chg*i_weight*0.01 from test",row.names=TRUE)
newdf
newdf<-sqldf("select sum(pct_chg*i_weight*0.01) from test",row.names=TRUE)
newdf
head(test)
??cor
dat<-read.table('SegData.csv',,sep=',',header=TRUE)
head(dat)
dat<-subset(dat,store_exp>0 & online_exp>0 & age<100)
cor(dat$age,dat$store_exp)
cor(dat$age,dat$online_exp,method="spearman")
cor(dat$age,dat$online_exp,method="kendall")
smoother <- loess(dat$online_exp ~ dat$age)
pred.smoother<-predict(smoother, dat$age)
rss<-sum((dat$online_exp-pred.smoother)^2)
tss<-sum((dat$online_exp-mean(dat$online_exp))^2)
(r2<-1-rss/tss)
library(lattice)
xyplot(dat$online_exp ~ dat$age,
type = c("p", "smooth"),
xlab = "age",
ylab = "online_exp")
library(ggplot2)
ggplot(dat,aes(age,online_exp))+geom_smooth()
TrainXtrans<-dat[,grep("Q",names(dat))]
TrainY<-dat$store_exp+dat$online_exp
loessResults <- filterVarImp(x = TrainXtrans, y = TrainY, nonpara = TRUE)
head(loessResults)
t.test(online_exp~gender,data=dat)
anova(lm(online_exp~segment,data=dat))
vars<-c("age","income","store_exp","online_exp")
x<-subset(dat,select=vars)
y<-dat$house
rocValues <- filterVarImp(x = x, y = y)
rocValues
y<-dat$segment
rocValues <- filterVarImp(x = x, y = y)
rocValues
library(CORElearn)
infoCore(what="attrEvalReg")
infoCore(what="attrEval")
dat<-read.table('SegData.csv',,sep=',',header=TRUE)
dat=filter(dat,!is.na(income))
xtrain=dat%>%
dplyr::select(num_range("Q", 1:10))
dat<-read.table('SegData.csv',,sep=',',header=TRUE)
dat=filter(dat,!is.na(income))
library(readr)
library(dplyr)
topic<-read.table('topic_new.csv',,sep=',',header=TRUE)
glimpse(topic)
posted_at2<-topic$posted_at
topic$year<-substr(posted_at2,1,4)
car::some(topic$year)
barplot(table(topic$year),family ="Songti SC", main="年度发帖数目频数直方图")
head(test)
d <- density(test$pct_chg)
subset(test,is.numeric(pct_chg))
subset(test,isnumeric(pct_chg))
subset(test,is.numeric(pct_chg))
subset(test,is.numeric(PCT_CHG))
subset(test,is.numeric(PCT_CHG)=0)
subset(test,is.numeric(PCT_CHG)<=0)
subset(test,is.numeric(PCT_CHG)>=0)
d <- density(test$pct_chg)
d <- density(as.numeric(test$pct_chg))
head(test)
d <- density(table(test$pct_chg))
boxplot(test$pct_chg, main="沪深300个股涨跌分布", ylab="个股涨跌")
head(mtcars)
head(test)
airline<-read.table('AirlineRating.csv',,sep=',',header=TRUE)
glimpse(airline)
head(airline)
dplyr::select(airline,Easy_Reservation:Recommend)%>%
# 得到相关矩阵
cor()%>%
# 用corrplot()绘制相关图
# 选项order="hclust"按照变量的相似度，基于系统聚类的结果对行列进行重新排列
corrplot(,order="hclust")
library(corrplot)
dplyr::select(airline,Easy_Reservation:Recommend)%>%
# 得到相关矩阵
cor()%>%
# 用corrplot()绘制相关图
# 选项order="hclust"按照变量的相似度，基于系统聚类的结果对行列进行重新排列
corrplot(,order="hclust")
airline.mean<-dplyr::select(airline,-ID)%>%
# 按Airline对数据进行分组总结
group_by(Airline)%>%
# 对每个数值
summarise_each(funs(mean))%>%
# 显示数据
glimpse()
head(airline.mean)
head(test)
row.names(test.new)<-test.new$sec_name
test.new<-test
row.names(test.new)<-test.new$sec_name
head(test.new)
boxplot(test.new$pct_chg, main="沪深300个股涨跌分布", ylab="个股涨跌")
summary(test.new$pct_chg)
boxplot(test$PCT_CHG, main="沪深300个股涨跌分布", ylab="个股涨跌")
d <- density(test.new$PCT_CHG)
plot(d)
library(pastecs)
install.packages(pastecs)
install.packages("pasctecs")
install.packages("pastecs")
library(pastecs)
stat.desc(test.new$PCT_CHG)
??stat.desc
stat.desc(test.new$PCT_CHG, basic=TRUE, desc=TRUE, norm=TRUE, p=0.95)
stat.desc(test.new$PCT_CHG, basic=TRUE, desc=TRUE, norm=TRUE, p=0.95)
library(psych)
describe(test.new$PCT_CHG)
save.image("C:/Users/samurai/Desktop/20180311.RData")
load("D:/quant/20180312.RData")
write.csv(d1, file = "c:/hs300.csv")
topic
topic<-read.table('topic_new.csv',,sep=',',header=TRUE)
mode(topic)
d2<-as.list(d1)
write.csv(d2, file = "c:/hs300.csv")
write.csv(d2, file = "c:/hs300.csv")
write.table(d2, file = "c:/hs300.csv")
write.table(d2, file = "hs300.csv")
summary(d1)
glimpse(d1)
install.packages("ggdendro")
fit.average <- hclust(test.1, method="average")
head(test.1)
nrow(test.1)
tail(test.1)
test.1<-dplyr::select(test.new,PCT_CHG)
fit.average <- hclust(test.1, method="average")
fit.average <- hclust(as.numeric(test.1), method="average")
test.1<-as.numeric(dplyr::select(test.new,PCT_CHG))
test.1<-dplyr::select(test.new,PCT_CHG)
test.1<-as.numeric(test.1)
d1 <- dist(test.1)
fit.average <- hclust(d1, method="average")
ggdendrogram(fit.average, size = 32, theme_dendro = FALSE, color = "tomato")
library(ggdendro)
ggdendrogram(fit.average, size = 32, theme_dendro = FALSE, color = "tomato")
plot(as.phylo(fit.average), type = "fan")
library(ggdendro)
plot(as.phylo(fit.average), type = "fan")
d1 <- dist(test.1)
fit.average <- hclust(d1, method="average")
plot(as.phylo(fit.average), type = "fan")
library(ggdendro)
ggdendrogram(fit.average, size = 32, theme_dendro = FALSE, color = "tomato")
install.packages(ape)
install.packages("ape")
plot(as.phylo(fit.average), type = "fan")
library(ape)
plot(as.phylo(fit.average), type = "fan")
plot(as.phylo(fit.average), type = "fan")
plot(as.phylo(fit.average,), type = "unrooted")
data(wine, package="rattle")
library(rattle)
install.packages("rattle")
data(wine, package="rattle")
library("rattle")
rattle()
data(wine, package="rattle")
library(NbClust)
install.packages("NbClust")
set.seed(1234)
devAskNewPage(ask=TRUE)
nc <- NbClust(test.1, min.nc=2, max.nc=15, method="kmeans")
library(NbClust)
nc <- NbClust(test.1, min.nc=2, max.nc=15, method="kmeans")
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]),
xlab="Number of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
set.seed(1234)
fit.km <- kmeans(test.1, 3, nstart=25)
fit.km$size
fit.km$centers
d <- dist(test.1)
as.matrix(d)[1:4,1:4]
write.table(d, file = "hs300_new.csv")
write.table(as.matrix(d), file = "hs300_new.csv")
write.table(as.matrix(d), file = "hs300_new.csv",sep = " ", row.names = TRUE,col.names = TRUE)
write.table(as.matrix(d)[1:4,1:4], file = "hs300_new.csv",sep = " ")
write.table(as.matrix(d)[1:4,1:4],file = "hs300_new.csv",row.names=F)
write.table(as.matrix(d)[1:4,1:4],file = "hs300_new.csv",header=T,fill=T,sep=" ")
write.table(as.matrix(d)[1:4,1:4],file = "hs300_new.csv",sep=" ")
write.table(as.matrix(d)[1:4,1:4],file = "hs300_new.csv",sep=" ")
write.table(as.matrix(d)[1:4,1:4],file = "hs300_new.csv", quote = FALSE, sep = ",")
setwd("C:/quant/introTS/chap6/")
setwd("C:/quant/introTS/chap6/")
setwd("C:/books/R/CRC.Data.Science.in.R/PairsTrading/")
setwd("D:/quant/introTS/chap6/")
da=read.table("taq-cat-t-jan042010.txt",header=T)
head(da)
nrow(da)
library(readr)
library(dplyr)
glimpse(da)
summary(da)
head(da)
vol=da$size/100
da1=read.table("taq-cat-cpch-jan042010.txt")
cpch=da1[,1]   # category of price change
head(cpch)
head(da1)
pch=da1[,2]  # price change
cf=as.factor(cpch)  # create categories in R
head(cf)
length(cf)
y=cf[4:37715]
y1=cf[3:37714]  # create indicator variables for lag-1 cpch
y2=cf[2:37713]  # create indicator variables for lag-2 cpch
vol=vol[2:37716]
v2=vol[2:37713] # create lag-2 volume
cp1=pch[3:37714] # select lagged price changes
cp2=pch[2:37713]; cp3=pch[1:37712]
library(MASS) # load package
m1=polr(y~v2+cp1+cp2+cp3+y1+y2,method="probit")
summary(m1)
names(m1)
yhat=m1$fitted.values
print(yhat[1:5,],digits=3)
da=read.table("taq-cat-cpch-jan042010.txt")
dim(da)
pch=da[,2]  # create Ai, Di, and Si and their lagged variables
idx=c(1:37715)[pch > 0]
jdx=c(1:37715)[pch < 0]
A=rep(0,37715); A[idx]=1; A[jdx]=1
D=rep(0,37715); D[idx]=1; D[jdx]=-1
S=abs(da[,1]-4)
Ai=A[2:37715]; Aim1=A[1:37714]
Di=D[2:37715]; Dim1=D[1:37714]
Si=S[2:37715]; Sim1=S[1:37714]
m1=glm(Ai~Aim1,family="binomial")
summary(m1)
di=Di[Ai==1]
dim1=Dim1[Ai==1]
di=(di+abs(di))/2 # transform di to binary
m2=glm(di~dim1,family="binomial")
summary(m2)
si=Si[Di==1]
sim1=Sim1[Di==1]
source("GeoSize.R") # R script to fit Geometric dist.
m3=GeoSize(si,sim1)
nsi=Si[Di==-1]
source("GeoSize.R") # R script to fit Geometric dist.
m3=GeoSize(si,sim1)
nsi=Si[Di==-1]
nsim1=Sim1[Di==-1]
m4=GeoSize(nsi,nsim1)
da=read.table("taq-cat-t-jan04t082010.txt",header=T)
head(da)
sec=3600*da$hour+60*da$minute+da$second # time in seconds
ist=3600*9+30*60;  end=3600*16
lunch=3600*12
length(sec)
idx=c(1:155267)[sec < ist]   # before market opens
jdx=c(1:155267)[sec > end]  # after market closes
sec=sec[-c(idx,jdx)]  # normal trading hours only.
length(sec)
dt=diff(sec)
kdx=c(1:length(dt))[dt > 0] # Positive durations only
length(kdx)
ti=sec[2:155077]
dt=dt[kdx]
ti=ti[kdx]
plot(dt,type='l',xlab='index',ylab='duration')
st=3600*6.5
f1=(ti-lunch)/st
ft=cbind(f1,f1^2)
m2=lm(log(dt)~ft)  # Linear model for log(durations)
summary(m2)
names(m2)
fit=m2$fitted.values
adjdt=dt/exp(fit)
adjdt
source("acd.R")
source("acd.R")
m2=acd(adjdt,order=c(1,1),cond.dist="exp")
names(m2)
m3=acd(adjdt,order=c(1,1),cond.dist="weibull")
m5=acd(adjdt,order=c(1,2),cond.dist="weibull")
source("acd.R")
m2=acd(adjdt,order=c(1,1),cond.dist="exp")
da=read.table("taq-cat-feb2010.txt",header=T)
m1=hfanal(da,1)
Rv2=cbind(m1$Ytot,m1$realized)
Ytot=c(Ytot,m1$Ytot)
Ntrad=c(Ntrad,m1$ntrad)
m2=hfanal(da,2)
Rv2=cbind(Rv2,m2$realized)
m3=hfanal(da,3)
Rv2=cbind(Rv2,m3$realized)
m4=hfanal(da,4)
Rv2=cbind(Rv2,m4$realized)
m5=hfanal(da,5)
Rv2=cbind(Rv2,m5$realized)
m6=hfanal(da,10)
Rv2=cbind(Rv2,m6$realized)
m7=hfanal(da,15)
Rv2=cbind(Rv2,m7$realized)
m8=hfanal(da,20)
Rv2=cbind(Rv2,m8$realized)
m9=hfanal(da,30)
Rv2=cbind(Rv2,m9$realized)
RV=rbind(Rv,Rv2)  # Combine Jan and Feb results
source("hf2ts.R")
da=read.table("taq-cat-may2010.txt",header=T)
m5=hf2ts(da,int=5)
names(m5)
da=read.table("taq-cat-apr2010.txt",header=T)
m4=hf2ts(da,int=5)
da=read.table("taq-cat-mar2010.txt",header=T)
m3=hf2ts(da,int=5)
da=read.table("taq-cat-feb2010.txt",header=T)
m2=hf2ts(da,int=5)
da=read.table("taq-cat-jan2010.txt",header=T)
m1=hf2ts(da,int=5)
Ytot=c(m1$Ytot,m2$Ytot,m3$Ytot,m4$Ytot,m5$Ytot)  # Consecutive trades
RV=c(m1$realized,m2$realized,m3$realized,m4$realized,m5$realized)
mRV=c(m1$ave.RV,m2$ave.RV,m3$ave.RV,m4$ave.RV,m5$ave.RV)
source("hf2ts.R")
da=read.table("taq-cat-may2010.txt",header=T)
m5=hf2ts(da,int=5)
names(m5)
da=read.table("taq-cat-apr2010.txt",header=T)
m4=hf2ts(da,int=5)
da=read.table("taq-cat-mar2010.txt",header=T)
m3=hf2ts(da,int=5)
da=read.table("taq-cat-feb2010.txt",header=T)
m2=hf2ts(da,int=5)
da=read.table("taq-cat-jan2010.txt",header=T)
m1=hf2ts(da,int=5)
Ytot=c(m1$Ytot,m2$Ytot,m3$Ytot,m4$Ytot,m5$Ytot)  # Consecutive trades
RV=c(m1$realized,m2$realized,m3$realized,m4$realized,m5$realized)
mRV=c(m1$ave.RV,m2$ave.RV,m3$ave.RV,m4$ave.RV,m5$ave.RV)
head(Ytot)
summary(Ytot)
plot(Ytot)
head(da)
mode(da)
library(rlist)
install.packages("rlist")
head(da)
list.order(da, (hour,min))
list.order(da, (hour))
list.order(da, (Hour))
library(rlist)
list.order(da, (Hour))
list.order(da,(Hour))
list.order(da,(da$Hour))
list.order(da,(da$Hour))
??list.order
head(da)
x <- list(p1 = list(type='A',score=list(c1=10,c2=8)),
p2 = list(type='B',score=list(c1=9,c2=9)),
p3 = list(type='B',score=list(c1=9,c2=7)))
head(x)
list.order(x, type, (score$c2))
list.order(x, min(score$c1,score$c2))
library(sqldf)
sqldf("select * from da order by Hour",
row.names=TRUE)
head(sqldf("select * from da order by Hour",  row.names=TRUE))
length(da)
nrow(da)
head(sqldf("select * from da order by Date,Hour,Min,Sec",  row.names=TRUE))
head(sqldf("select * from da order by Date,Hour,Min,Sec",  row.names=TRUE))
tail(sqldf("select * from da order by Date,Hour,Min,Sec",  row.names=TRUE))
head(m1)
head(Rv2)
Rv2=cbind(m1$Ytot,m1$realized)
Rv2
head(Rv)
Rv=cbind(Ytot,m1$realized)
m1=hfanal(da,1)
da=read.table("taq-cat-jan2010.txt",header=T)
m1=hfanal(da,1)
source("hfanal.R")
m1=hfanal(da,1)
names(m1)
Ytot=m1$Ytot
Ntrad=m1$ntrad
Rv=cbind(Ytot,m1$realized)
head(Rv)
